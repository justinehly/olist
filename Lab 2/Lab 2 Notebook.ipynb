{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df3e3bf",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## Group Members\n",
    "- Helene Barrera\n",
    "- Feby Thomas\n",
    "- John Olanipekun\n",
    "- Justin Ehly\n",
    "\n",
    "## <span style='color:blue'> Mini Lab Executive Summary </span>\n",
    "* This exercise seeks to fit:\n",
    "    - Two classification tasks OR\n",
    "    - Two regression tasks OR\n",
    "    - One classification task and one regression task\n",
    ". \n",
    "* We will develop a base classifier that determines if a package is <span style='color:red'> **'late', 'on time' or 'early'**</span> and subsequently tune the hyperparameters for possible improvement to the performance metrics (i.e. Accuracy, Precision, F-Score and Sensitivity).\n",
    "* This modeling phase will utilize the 'clean' dataset that was processed for the visualization task, but still needs some data preprocessing due to the transfer from one project to the next using a CSV file. \n",
    "* The **derived** response for the objective is actual delivery duration. Therefore the predictive model will predict actual delivery duration with performance measured on the held out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9404a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0b9f56",
   "metadata": {},
   "source": [
    "# Data Preparation (15 points total)\n",
    "* [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "*[5 points] Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfc811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9cb635",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation (70 points total)\n",
    "* [10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "* [10 points] Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "* [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "* [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "* [10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "* [10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f5056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fa948a",
   "metadata": {},
   "source": [
    "# Deployment (5 points total)\n",
    "* How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would you deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea7d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fa4159d",
   "metadata": {},
   "source": [
    "# Exceptional Work (10 points total)\n",
    "* You have free reign to provide additional analyses.\n",
    "* One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db20b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
