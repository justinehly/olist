{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c71700",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style='color:blue'> **1. Create Model (50 points)** </span>\n",
    "---\n",
    "\n",
    "- Create a **logistic regression model** and a **support vector machine model** for the **classification task** involved with your dataset.  \n",
    "\n",
    "- Assess how well each model performs (use 80/20 training/testing split for your data).\n",
    "    - <span style='color:green'> ***80/20 Split performed during preprocessing above***</span>\n",
    "\n",
    "- Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. \n",
    "    - That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines.\n",
    "            -For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b35f61",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> Create a Logistic Regression classifier </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3662c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Logistic regression\n",
    "logr_clf = LogisticRegression(penalty='l2', #default\n",
    "                              C=1, #default \n",
    "                              class_weight='balanced', # use with imbalanced dataset\n",
    "                              solver='newton-cg', # only solver that works with this dataset\n",
    "                              multi_class='multinomial', \n",
    "                              random_state=42) \n",
    "logr_clf.fit(X_train_class,olist_train_y_range)\n",
    "\n",
    "# Note: For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; \n",
    "# ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "# Note: solver default lbfgs, sag and saga did not work, we hit max_iter and even set at 4000 it still did not work\n",
    "# Note: only newton-cg works\n",
    "\n",
    "#y_train_pred_class = sgd_clf.predict(X_train_class)\n",
    "yhat_lr = logr_clf.predict(X_test_class)\n",
    "\n",
    "print('Logistic Regression Metrics:')\n",
    "print(logr_clf)\n",
    "print(classification_report(olist_test_y_range, yhat_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820933d",
   "metadata": {},
   "source": [
    "## <span style='color:blue'>Utilize RandomizedSearchCV to tune our Logistic Regression Model</span>\n",
    "\n",
    "source: https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_random_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "X = X_train_class\n",
    "y = olist_train_y_range\n",
    "# create logistic Regression\n",
    "lgr = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "# create regularization penalty space\n",
    "penalty = ['l2','none']  #only penalties that work with newton-cg\n",
    "\n",
    "# create regulatization hyperparameter distribution using uniform distribution\n",
    "C = uniform(loc=0, scale=4) \n",
    "\n",
    "# create hyperparameter options\n",
    "hyp = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create random search, 5fold CV, 100 iterations\n",
    "clf = RandomizedSearchCV(lgr, hyp, random_state=42, n_iter=10, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "#fit random search\n",
    "best_model = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb35e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view hyperparameter values of best model\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best Score: ', best_model.best_score_) \n",
    "print('Best Params: ', best_model.best_params_)\n",
    "print('Best Time (seconds): ', best_model.refit_time_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe561c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using best model\n",
    "yhat = best_model.predict(X_test_class)\n",
    "\n",
    "print('Best RandomSearchCV Logistic Regression Metrics:')\n",
    "print(classification_report(olist_test_y_range, yhat, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adf93c",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> 2. Create a linear SVM classifier with stochastic descent </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SVM Model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train_class, olist_train_y_range)\n",
    "\n",
    "#y_train_pred_class = sgd_clf.predict(X_train_class)\n",
    "yhat_svm = sgd_clf.predict(X_test_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with SD Metrics\n",
    "print('Logistic Regression Metrics:')\n",
    "print(sgd_clf)\n",
    "print(classification_report(olist_test_y_range, yhat_svm, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f4a95",
   "metadata": {},
   "source": [
    "## <span style='color:blue'>Utilize RandomizedSearchCV to tune our SVM Model</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1723eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "# create variable dictionaries\n",
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge']\n",
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive'] \n",
    "eta0 = [1, 10, 100] \n",
    "\n",
    "# create hyperparameter options\n",
    "param_distributions = dict(loss=loss, \n",
    "                           penalty=penalty, \n",
    "                           alpha=alpha, \n",
    "                           learning_rate=learning_rate, \n",
    "                           eta0=eta0)\n",
    "\n",
    "# create the classifier\n",
    "sgd = SGDClassifier(early_stopping=True, validation_fraction=0.15, max_iter=100, class_weight = \"balanced\") \n",
    "\n",
    "# create RandomizedSearchCV\n",
    "random = RandomizedSearchCV(estimator=sgd,\n",
    "                            param_distributions=param_distributions, \n",
    "                            verbose=1, \n",
    "                            n_iter=100, \n",
    "                            n_jobs=-1) \n",
    "random_result = random.fit(X_train_class,olist_train_y_range) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Best Results\n",
    "print('Best Score: ', random_result.best_score_) \n",
    "print('Best Params: ', random_result.best_params_)\n",
    "print('Best Time (seconds): ', random_result.refit_time_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using best model\n",
    "yhat_rand_svm = random_result.predict(X_test_class)\n",
    "\n",
    "print('Best Random SVM SD Metrics:')\n",
    "print(classification_report(olist_test_y_range, yhat_rand_svm, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e98ec",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> 2. Model Advantages (10 points) </span>\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "\n",
    "**Discussion:**\n",
    "* We ran 2 basic models using only minimal tuning for multinomial classification and 2 refitted models using RandonSearchCV for tuning.\n",
    "* We used multinomial classification \n",
    "* Basic Logistical Regression is the only model that was able to categorize deliveries that were made on-time (value #2) with a precision of 3%. \n",
    "* Refitted Logistical Regression: Once we tuned the model using RandomizedSearchCV with a 5-fold validation, the best model did not correctly classify any on time deliveries; however, the best fit model did increase the precision for classifying late deliveries from 64% to 79%! We also found the best model increased overall accuracy from 75% to 94%, but again, not correctly categorizing any of the on time deliveries. \n",
    "* SVM with Stochastic Descent with minor tuning to handle multinormial classification returned an overall accuracy of 94%, but failed to correctly classify any on time packages. This model did present the best precision for categorizing late deliveries at 85%, but fell short of the classification on early deliveries to the basic logistical regression model 95% to 98%.\n",
    "* Reffited SVM with Stochastic Descent: we again utilized RandomSearchCV to tune our SVM with stochastic decent and found that the resulting best model had the fastest time of 0.72seconds, while maintaining an overall accuracy of 94% that was in line with both the refitted logistical regression model and the initial SVM stochastic descent model. This model was also not able to correctly categorize on time deliveries and fell short of the untuned SVM model's precision of late deliveries...it did have better recall on the SVM's categorization of late deliveries. This model also had the best overall performance of identifying early deliveries based on the ratio between the precision and the recall that was rounded in this report, but from looking at the raw numbers it has a slightly better ratio than the other models.\n",
    "* Time - we found the best time was the best fit SVM stochastic descent model at 0.71seconds compared to the best fit Logistical Regression model at just over 9 seconds.\n",
    "* Overall the best performing model was the refit SVM stochastic descent model, even thought it failed to correctly categorize any ontime deliveries. To improve this performance we should probaly look at breaking the categories into more equal sizes that cover a similiar time frame in terms of days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65684907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n================================================================\\n')\n",
    "print('=== Logistic Regression Metrics using minor tuning for class_weight, solver ===')\n",
    "print(logr_clf)\n",
    "print(classification_report(olist_test_y_range, yhat_lr))\n",
    "\n",
    "print('\\n================================================================\\n')\n",
    "print('=== Best RandomSearchCV Logistic Regression Metrics ===\\n')\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best Score: ', best_model.best_score_) \n",
    "print('Best Params: ', best_model.best_params_)\n",
    "print('Best Time (seconds): ', best_model.refit_time_)\n",
    "print(classification_report(olist_test_y_range, yhat, zero_division=0))\n",
    "\n",
    "print('\\n================================================================\\n')\n",
    "print('=== SVM with Stochastic Descent  Metrics ===\\n')\n",
    "print(classification_report(olist_test_y_range, yhat_svm, zero_division=0))\n",
    "\n",
    "print('\\n================================================================\\n')\n",
    "print('=== Best RandomSeachCV SVM-SD Metrics ===\\n')\n",
    "print('Best Score: ', random_result.best_score_) \n",
    "print('Best Params: ', random_result.best_params_)\n",
    "print('Best Time (seconds): ', random_result.refit_time_)\n",
    "print(classification_report(olist_test_y_range, yhat_rand_svm, zero_division=0))\n",
    "\n",
    "### <span style='color:blue'> **Discssion** </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98f3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30370ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1cfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b713b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
